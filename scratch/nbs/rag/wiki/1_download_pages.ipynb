{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import os\n",
    "\n",
    "def fetch_page(page_title: str, output_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the page content from Wikipedia, saves it in an HTML file in the specified directory, and returns the file name.\n",
    "    \"\"\"\n",
    "    wiki_html = wikipediaapi.Wikipedia(\n",
    "        user_agent='Tinker/0.1 (kartikeyapophali@gmail.com)',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.HTML\n",
    "    )\n",
    "\n",
    "    p_html = wiki_html.page(page_title)\n",
    "    underscored_page_title = page_title.replace(\" \", \"_\")\n",
    "    file_name = f\"{underscored_page_title}.html\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(p_html.text)\n",
    "        \n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def update_checkpoint(checkpoint_file: str, page_title: str, file_name: str):\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'w') as file:\n",
    "            json.dump({}, file, indent=4)\n",
    "    \n",
    "    with open(checkpoint_file, 'r+') as file:\n",
    "        checkpoint_data = json.load(file)\n",
    "        checkpoint_data[page_title] = file_name\n",
    "        file.seek(0)\n",
    "        json.dump(checkpoint_data, file, indent=4)\n",
    "        file.truncate()\n",
    "\n",
    "def load_checkpoint(checkpoint_file: str):\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'w') as file:\n",
    "            json.dump({}, file, indent=4)\n",
    "    with open(checkpoint_file, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory: str):\n",
    "    response_file = os.path.join(directory, 'response.json')\n",
    "    checkpoint_file = os.path.join(directory, 'page_title_file_name_map.json')\n",
    "    \n",
    "    with open(response_file, 'r') as file:\n",
    "        response_data = json.load(file)\n",
    "    \n",
    "    checkpoint_data = load_checkpoint(checkpoint_file)\n",
    "    processed_pages = set(checkpoint_data.keys())\n",
    "    \n",
    "    for member in response_data['query']['categorymembers']:\n",
    "        if member['ns'] == 0:  # Only process pages, not categories\n",
    "            page_title = member['title']\n",
    "            if page_title not in processed_pages:\n",
    "                try:\n",
    "                    file_name = fetch_page(page_title, directory)\n",
    "                    update_checkpoint(checkpoint_file, page_title, file_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing page {page_title}: {e}\")\n",
    "\n",
    "    for member in response_data['query']['categorymembers']:\n",
    "        if member['ns'] == 14:  # Process subcategories\n",
    "            subcategory_dir = os.path.join(directory, member['title'].replace('Category:', ''))\n",
    "            if os.path.isdir(subcategory_dir):\n",
    "                process_directory(subcategory_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory('Dinosaurs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Satisfactory result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "- Fetching 127 pages requires around 93 seconds in above experiment. 3800 pages will take close to an hour.\n",
    "- Performance can be optimized with batch processing. Experiments were done, but the underlying pages fetched were not in satisfactory format.\n",
    "- As downloading and indexing will not be done often, 1 hour of download time can be accepted as a trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(\"data/Dinosaurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested above with partial runs as well where some of the files were arbitrarily deleted to test checkpointing. The checkpointing file was created where it did not exist and the files were fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(\"data/Dinosaurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above run was expected to be fast. All files already exist.\n",
    "\n",
    "Next, just delete a few entries from the ```page_title_file_name_map``` and delete the corresponding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(\"data/Dinosaurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works! Three arbitrary files were removed along with their corresponding entries in the map. Upon rerun of function, the files were reinstated with their mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
