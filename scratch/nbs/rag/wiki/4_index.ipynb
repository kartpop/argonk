{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.0 at grpc_health/v1/health.proto. Please avoid checked-in Protobuf gencode that can be obsolete.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack_integrations.document_stores.weaviate.document_store import WeaviateDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_wiki_data(category: str, filepath: str, category_pages_indexed: dict) -> int:\n",
    "    \"\"\"\n",
    "    Indexes already chunked wiki data for all pages in a category and its subcategories. Chunked data is available in \n",
    "    the .metadata/chunk directory. The intermediate embeddings are stored in the .metadata/index/embeddings directory.\n",
    "    \n",
    "    List of Haystack Document objects is created from stored chunks and stored into three databases:\n",
    "    - ElasticsearchDocumentStore: for full-text search (list of Document objects without embeddings is stored)\n",
    "    - WeaviateDocumentStore: for vector search (list of Document objects enriched with embeddings is stored)\n",
    "    - Neo4j: for graph search (list of Document objects are stored as Chunk type nodes and Section, Page, Category type nodes\n",
    "    are created to represent the structure of the data)\n",
    "    \n",
    "    pseuodocode:\n",
    "    - get pages and subcategories from title_pathname map\n",
    "    - create pages_filename_set and categories_dirname_set\n",
    "    \n",
    "    - for each page in pages:\n",
    "        - if page is not already indexed (check in redis set):\n",
    "        - if page not in pages_filename_set:\n",
    "            - call get_chunks to get list of Document objects from chunks\n",
    "            - store documents in ElasticsearchDocumentStore (DocumentWriter(document_store = document_store, policy=DuplicatePolicy.SKIP)\n",
    "            - call get_embedded_documents to get list of Document objects with embeddings\n",
    "            - store documents in WeaviateDocumentStore (DocumentWriter(document_store = document_store, policy=DuplicatePolicy.SKIP)\n",
    "            - call create_graph to create graph representation of the data in Neo4j -- ensure duplicate nodes and edges not created\n",
    "            - add page to redis set\n",
    "    \n",
    "    - for each subcategory in categories:\n",
    "        - if subcategory is not already indexed (check in redis set):\n",
    "        - if subcategory not in categories_dirname_set:\n",
    "            - call index_wiki_data recursively on subcategory\n",
    "            - add subcategory to redis set\n",
    "        \n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_graph(category: str, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a graph representation of the category and connections to its subcategories and pages. The graph is created \n",
    "    on top of the individual page hierarchy graphs already existing in Neo4j. \n",
    "    \n",
    "    pseuodocode:\n",
    "    - get pages and subcategories from title_pathname map\n",
    "    \n",
    "    - for each page in pages:\n",
    "        - call create_category_to_page relationship\n",
    "        \n",
    "    - for each subcategory in categories:\n",
    "        - call create_category_to_subcategory relationship\n",
    "        - call create_category_graph recursively on subcategory\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import redis\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from haystack import Document\n",
    "from typing import Tuple\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "w_store = WeaviateDocumentStore(url=\"http://localhost:8088\")\n",
    "w_writer = DocumentWriter(document_store=w_store), policy=DuplicatePolicy.SKIP\n",
    "e_store = ElasticsearchDocumentStore(hosts= \"http://localhost:9200\")\n",
    "e_writer = DocumentWriter(document_store=e_store, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "def get_title_pathname_map(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the title_pathname map from a file. Returns an empty dictionary if the file does not exist.\n",
    "    \"\"\"\n",
    "    title_pathname_filepath = os.path.join(filepath, \".metadata/download/title_pathname.json\")\n",
    "    if not os.path.exists(title_pathname_filepath):\n",
    "        return {\"pages\": {}, \"categories\": {}}\n",
    "    \n",
    "    with open(title_pathname_filepath, \"r\") as file:\n",
    "        title_pathname = json.load(file)\n",
    "    \n",
    "    return title_pathname\n",
    "\n",
    "def get_documents_and_page_hierarchy(filepath: str, page_title: str, page_filename: str) -> Tuple[List[Document], dict]:\n",
    "    \"\"\"\n",
    "    Extracts the documents and hierarchy of a page from the stored chunks in the .metadata/chunk/{page_filename}.json file.\n",
    "    \"\"\"\n",
    "    chunk_filepath = os.path.join(filepath, \".metadata/chunk\", f\"{page_filename}.json\")\n",
    "    if not os.path.exists(chunk_filepath):\n",
    "        return [], {}\n",
    "    \n",
    "    with open(chunk_filepath, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    if not \"splitter\" in data:\n",
    "        return [], {}\n",
    "    \n",
    "    documents = []\n",
    "    if \"documents\" in data:\n",
    "        documents = data[\"documents\"]\n",
    "        documents = [Document.from_dict(doc) for doc in documents]  # convert dict to Haystack Document object\n",
    "\n",
    "    hierarchy = {}\n",
    "    if \"hierarchy\" in data:\n",
    "        hierarchy = data[\"hierarchy\"]\n",
    "    \n",
    "    return documents, hierarchy\n",
    "\n",
    "\n",
    "def store_documents_elasticsearch(documents: List[Document]) -> None:\n",
    "    \"\"\"\n",
    "    Store documents in ElasticsearchDocumentStore.\n",
    "    \"\"\"\n",
    "    e_writer.run(documents=documents)\n",
    "    \n",
    "def get_embedded_documents(documents: List[Document], filepath: str, page_filename: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Get embeddings for the documents using the OpenAIDocumentEmbedder.\n",
    "    \n",
    "    Store the embedded documents at .metadata/index/embeddings/{page_filename}.json\n",
    "    \"\"\"\n",
    "    embedded_documents = embedder.run(documents=documents)\n",
    "    \n",
    "    # Store the embedded documents\n",
    "    embeddings_filepath = os.path.join(filepath, \".metadata/index/embeddings\", f\"{page_filename}.json\")\n",
    "    embedded_docs_file_to_save = {\n",
    "        \"documents\": [doc.to_dict() for doc in embedded_documents[\"documents\"]],    # convert Haystack Document object to dict\n",
    "        \"meta\": embedded_documents[\"meta\"]\n",
    "        }\n",
    "    with open(embeddings_filepath, \"w\") as file:\n",
    "        json.dump(embedded_docs_file_to_save, file)\n",
    "    \n",
    "    return embedded_documents[\"documents\"]\n",
    "\n",
    "def store_documents_weaviate(documents: List[Document]) -> None:\n",
    "    \"\"\"\n",
    "    store documents in WeaviateDocumentStore.\n",
    "    \"\"\"\n",
    "    w_writer.run(documents=documents)\n",
    "    \n",
    "\n",
    "def create_graph(documents: List[dict]) -> None:\n",
    "    \"\"\"\n",
    "    Placeholder function to create graph representation of the data in Neo4j.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def index_wiki_data(category: str, filepath: str, category_pages_indexed: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    Indexes already chunked wiki data for all pages in a category and its subcategories. Chunked data is available in \n",
    "    the .metadata/chunk directory. The intermediate embeddings are stored in the .metadata/index/embeddings directory.\n",
    "    \n",
    "    List of Haystack Document objects is created from stored chunks and stored into three databases:\n",
    "    - ElasticsearchDocumentStore: for full-text search (list of Document objects without embeddings is stored)\n",
    "    - WeaviateDocumentStore: for vector search (list of Document objects enriched with embeddings is stored)\n",
    "    - Neo4j: for graph search (list of Document objects are stored as Chunk type nodes and Section, Page, Category type nodes\n",
    "    are created to represent the structure of the data)\n",
    "    \"\"\"\n",
    "    title_pathname = get_title_pathname_map(filepath)\n",
    "    \n",
    "    pages_filename_set = {file.name for file in Path(filepath).glob(\"*.html\")}\n",
    "    categories_dirname_set = {dir.name for dir in Path(filepath).iterdir() if dir.is_dir() and dir.name != \".metadata\"}\n",
    "    \n",
    "    num_total_pages_indexed = 0\n",
    "    \n",
    "    pages = title_pathname[\"pages\"]\n",
    "    for page_title, page_filename in pages.items():\n",
    "        if r.sismember(\"indexed_pages\", page_title):\n",
    "            continue\n",
    "        if page_filename not in pages_filename_set:\n",
    "            continue\n",
    "        documents, hierarchy = get_documents_and_page_hierarchy(filepath, page_title, page_filename)\n",
    "        store_documents_elasticsearch(documents)\n",
    "        embedded_documents = get_embedded_documents(documents)\n",
    "        store_documents_weaviate(embedded_documents)\n",
    "        create_graph(hierarchy)\n",
    "        r.sadd(\"indexed_pages\", page_title)\n",
    "        num_total_pages_indexed += 1\n",
    "    \n",
    "    if num_total_pages_indexed > 0:\n",
    "        category_pages_indexed[category] = num_total_pages_indexed\n",
    "    \n",
    "    subcategories = title_pathname[\"categories\"]\n",
    "    for subcategory_title, subcategory_path in subcategories.items():\n",
    "        if r.sismember(\"indexed_categories\", subcategory_title):\n",
    "            continue\n",
    "        if subcategory_path not in categories_dirname_set:\n",
    "            continue\n",
    "        subcategory_path = os.path.join(filepath, subcategory_path)\n",
    "        num_total_pages_indexed += index_wiki_data(subcategory_title, subcategory_path, category_pages_indexed)\n",
    "        r.sadd(\"indexed_categories\", subcategory_title)\n",
    "    \n",
    "    return num_total_pages_indexed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
