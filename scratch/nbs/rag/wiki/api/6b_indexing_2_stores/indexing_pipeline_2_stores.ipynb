{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def WikiPageChunks(html_str: str) -> List:\n",
    "    soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    def clean_text(text):\n",
    "        cleaned_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    html_soup = soup.body or soup\n",
    "    nested = ['ul', 'ol', 'dl', 'li', 'dt', 'dd']\n",
    "    for tag in html_soup.find_all(recursive=False):\n",
    "        if tag.name == 'p':\n",
    "            chunks.append(clean_text(tag.get_text(separator=' ')))\n",
    "        elif tag.name == 'link':\n",
    "            continue\n",
    "        elif tag.name in nested:\n",
    "            list_items = tag.find_all('li')\n",
    "            list_text = ' '.join([f\"- {clean_text(li.get_text(separator=' '))}\" for li in list_items])\n",
    "            chunks.append(list_text)\n",
    "        else:\n",
    "            chunks.append(str(tag))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack import component\n",
    "import uuid\n",
    "\n",
    "@component\n",
    "class WikiPageChunker:\n",
    "    \"\"\"\n",
    "    A component that splits the content of Wikipedia pages into chunks.\n",
    "    The document content is expected to be in HTML format fetched via wikipediaapi and\n",
    "    which has been run through TextFileToDocument converter.\n",
    "    \"\"\"\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]):\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            html_content = doc.content\n",
    "            page_chunks = WikiPageChunks(html_content)\n",
    "            i = 0\n",
    "            for chunk in page_chunks:\n",
    "                if chunk != \"\":\n",
    "                    chunks.append(\n",
    "                        Document(\n",
    "                            id=str(uuid.uuid4()),\n",
    "                            content=chunk,\n",
    "                            meta={\"file_path\": doc.meta[\"file_path\"], \n",
    "                                \"source_id\": doc.id,\n",
    "                                \"split_id\": i}\n",
    "                        )\n",
    "                    )\n",
    "                    i += 1\n",
    "        \n",
    "        return {\"documents\": chunks} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.0 at grpc_health/v1/health.proto. Please avoid checked-in Protobuf gencode that can be obsolete.\n",
      "  warnings.warn(\n",
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/posthog/client.py:345: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().replace(tzinfo=tzutc())\n",
      "Calculating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/posthog/request.py:40: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  body[\"sentAt\"] = datetime.utcnow().replace(tzinfo=tzutc()).isoformat()\n",
      "Calculating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-3-small',\n",
       "   'usage': {'prompt_tokens': 695, 'total_tokens': 695}}},\n",
       " 'e_writer': {'documents_written': 4},\n",
       " 'w_writer': {'documents_written': 4}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack_integrations.document_stores.weaviate.document_store import WeaviateDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "\n",
    "converter = TextFileToDocument()\n",
    "splitter = WikiPageChunker()\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "w_store = WeaviateDocumentStore(url=\"http://localhost:8088\")\n",
    "w_writer = DocumentWriter(document_store=w_store)\n",
    "e_store = ElasticsearchDocumentStore(hosts= \"http://localhost:9200\")\n",
    "e_writer = DocumentWriter(document_store=e_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "indexing_pipeline.add_component(\"converter\", converter)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"w_writer\", w_writer)\n",
    "indexing_pipeline.add_component(\"e_writer\", e_writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"w_writer\")\n",
    "indexing_pipeline.connect(\"splitter\", \"e_writer\")\n",
    "\n",
    "indexing_pipeline.run(data={\"converter\": {\"sources\": [Path(\"dinosaur-page.html\")]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(e_store.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(w_store.count_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Can one indexing pipeline be used to write into 2 store?\n",
    "\n",
    "Answer: Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Can the documents in both stores be correlated? Will tehy have the same ids?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.components.retrievers.elasticsearch import ElasticsearchBM25Retriever\n",
    "\n",
    "eretriever = ElasticsearchBM25Retriever(document_store=e_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.components.retrievers.weaviate.bm25_retriever import WeaviateBM25Retriever\n",
    "\n",
    "w_retriever = WeaviateBM25Retriever(document_store=w_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Dinosaurs\"\n",
    "\n",
    "elastic_fetched = eretriever.run(query=question, top_k=4)\n",
    "weaviate_fetched = w_retriever.run(query=question, top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(id=d592ed23-4543-4a02-85ea-000d1d408ac3, content: 'Dinosaurs are a diverse group of reptiles of the clade Dinosauria . They first appeared during the T...', meta: {'file_path': 'dinosaur-page.html', 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'split_id': 0}, score: 0.37987244, embedding: vector of size 1536),\n",
       "  Document(id=218342f4-2cc2-467f-9ced-4370add023a9, content: 'The first dinosaur fossils were recognized in the early 19th century, with the name \"dinosaur\" (mean...', meta: {'file_path': 'dinosaur-page.html', 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'split_id': 3}, score: 0.31545743, embedding: vector of size 1536),\n",
       "  Document(id=d52a5246-5c61-499d-905b-7be544efb4b5, content: 'While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some...', meta: {'file_path': 'dinosaur-page.html', 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'split_id': 2}, score: 0.2792654, embedding: vector of size 1536),\n",
       "  Document(id=ae3a9483-fcfe-4ce5-a818-058e1b1dac5d, content: 'Dinosaurs are varied from taxonomic, morphological and ecological standpoints. Birds, at over 11,000...', meta: {'file_path': 'dinosaur-page.html', 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'split_id': 1}, score: 0.20077062, embedding: vector of size 1536)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(id=ae3a9483-fcfe-4ce5-a818-058e1b1dac5d, content: 'Dinosaurs are varied from taxonomic, morphological and ecological standpoints. Birds, at over 11,000...', meta: {'split_id': 1.0, 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'file_path': 'dinosaur-page.html'}, score: 0.09117916226387024, embedding: vector of size 1536),\n",
       "  Document(id=d592ed23-4543-4a02-85ea-000d1d408ac3, content: 'Dinosaurs are a diverse group of reptiles of the clade Dinosauria . They first appeared during the T...', meta: {'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'split_id': 0.0, 'file_path': 'dinosaur-page.html'}, score: 0.09064868092536926, embedding: vector of size 1536),\n",
       "  Document(id=d52a5246-5c61-499d-905b-7be544efb4b5, content: 'While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some...', meta: {'split_id': 2.0, 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'file_path': 'dinosaur-page.html'}, score: 0.08679258823394775, embedding: vector of size 1536),\n",
       "  Document(id=218342f4-2cc2-467f-9ced-4370add023a9, content: 'The first dinosaur fossils were recognized in the early 19th century, with the name \"dinosaur\" (mean...', meta: {'split_id': 3.0, 'source_id': 'beae8a4fccec875985e8817cfdb53011adcf64cf1e57ba160f710fdf6641cccb', 'file_path': 'dinosaur-page.html'}, score: 0.06719280034303665, embedding: vector of size 1536)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, documents in the two stores will have the same IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
