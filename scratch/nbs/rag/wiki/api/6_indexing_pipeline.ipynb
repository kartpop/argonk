{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def WikiPageChunks(html_str: str) -> List:\n",
    "    soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    def clean_text(text):\n",
    "        cleaned_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    html_soup = soup.body or soup\n",
    "    nested = ['ul', 'ol', 'dl', 'li', 'dt', 'dd']\n",
    "    for tag in html_soup.find_all(recursive=False):\n",
    "        if tag.name == 'p':\n",
    "            chunks.append(clean_text(tag.get_text(separator=' ')))\n",
    "        elif tag.name == 'link':\n",
    "            continue\n",
    "        elif tag.name in nested:\n",
    "            list_items = tag.find_all('li')\n",
    "            list_text = ' '.join([f\"- {clean_text(li.get_text(separator=' '))}\" for li in list_items])\n",
    "            chunks.append(list_text)\n",
    "        else:\n",
    "            chunks.append(str(tag))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack import component\n",
    "import uuid\n",
    "\n",
    "@component\n",
    "class WikiPageChunker:\n",
    "    \"\"\"\n",
    "    A component that splits the content of Wikipedia pages into chunks.\n",
    "    The document content is expected to be in HTML format fetched via wikipediaapi and\n",
    "    which has been run through TextFileToDocument converter.\n",
    "    \"\"\"\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]):\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            html_content = doc.content\n",
    "            page_chunks = WikiPageChunks(html_content)\n",
    "            i = 0\n",
    "            for chunk in page_chunks:\n",
    "                if chunk != \"\":\n",
    "                    chunks.append(\n",
    "                        Document(\n",
    "                            id=str(uuid.uuid4()),\n",
    "                            content=chunk,\n",
    "                            meta={\"file_path\": doc.meta[\"file_path\"], \n",
    "                                \"source_id\": doc.id,\n",
    "                                \"split_id\": i}\n",
    "                        )\n",
    "                    )\n",
    "                    i += 1\n",
    "        \n",
    "        return {\"documents\": chunks} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.0 at grpc_health/v1/health.proto. Please avoid checked-in Protobuf gencode that can be obsolete.\n",
      "  warnings.warn(\n",
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/posthog/client.py:345: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().replace(tzinfo=tzutc())\n",
      "Calculating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/posthog/request.py:40: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  body[\"sentAt\"] = datetime.utcnow().replace(tzinfo=tzutc()).isoformat()\n",
      "Calculating embeddings: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-3-small',\n",
       "   'usage': {'prompt_tokens': 20925, 'total_tokens': 20925}}},\n",
       " 'writer': {'documents_written': 135}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack_integrations.document_stores.weaviate.document_store import WeaviateDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "\n",
    "converter = TextFileToDocument()\n",
    "splitter = WikiPageChunker()\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "document_store = WeaviateDocumentStore(url=\"http://localhost:8088\")\n",
    "document_writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "indexing_pipeline.add_component(\"converter\", converter)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", document_writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "\n",
    "indexing_pipeline.run(data={\"converter\": {\"sources\": [Path(\"dinosaur-page.html\")]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "print(document_store.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
