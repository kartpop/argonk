{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk all wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'wiki' to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack import Pipeline\n",
    "from wiki.lib.index.chunk.wiki_page_chunker import WikiPageChunker\n",
    "import redis\n",
    "import json\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_pathname_map(filepath: str) -> dict:\n",
    "    metadata_download_path = os.path.join(filepath, \".metadata/download\")\n",
    "\n",
    "    title_pathname_filepath = os.path.join(\n",
    "        metadata_download_path, \"title_pathname.json\"\n",
    "    )\n",
    "    if not os.path.exists(title_pathname_filepath):\n",
    "        return {\"pages\": {}, \"categories\": {}}\n",
    "    \n",
    "    with open(title_pathname_filepath, \"r\") as file:\n",
    "        title_pathname = json.load(file)\n",
    "\n",
    "    return title_pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_serializable(result: dict) -> None:\n",
    "    \"\"\"\n",
    "    The result dictionary has objects of type 'Document' dataclass which is specific to Haystack. Those objects should be \n",
    "    flattened to a dictionary so that result dict can be serialized to JSON.\n",
    "    \"\"\"\n",
    "    documents = result[\"splitter\"][\"documents\"]\n",
    "    result[\"splitter\"][\"documents\"] = [doc.to_dict() for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_page(filepath: str, page_title: str, page_filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Chunk the page and store chunks in .metdata/chunks folder\n",
    "    \"\"\"\n",
    "    page_filepath = os.path.join(filepath, page_filename)\n",
    "    if not os.path.exists(page_filepath):\n",
    "        return\n",
    "\n",
    "    # Run chunk pipeline\n",
    "    converter = TextFileToDocument()\n",
    "    splitter = WikiPageChunker()\n",
    "    \n",
    "    chunk_pipeline = Pipeline()\n",
    "\n",
    "    chunk_pipeline.add_component(\"converter\", converter)\n",
    "    chunk_pipeline.add_component(\"splitter\", splitter)\n",
    "\n",
    "    chunk_pipeline.connect(\"converter\", \"splitter\")\n",
    "    \n",
    "    result = chunk_pipeline.run(data={\"converter\": {\"sources\": [Path(page_filepath)], \"meta\": {\"page_title\": page_title}}})\n",
    "    \n",
    "    make_result_serializable(result)\n",
    "    \n",
    "    # Write chunk results to file\n",
    "    metadata_chunk_path = os.path.join(filepath, \".metadata/chunk\")\n",
    "    if not os.path.exists(metadata_chunk_path):\n",
    "        os.makedirs(metadata_chunk_path)\n",
    "    page_chunk_filepath = os.path.join(\n",
    "        metadata_chunk_path, f\"{page_filename.replace(\".html\", \"\")}.json\"\n",
    "    )\n",
    "    with open(page_chunk_filepath, \"w\") as file:\n",
    "        json.dump(result, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_wiki_data(category: str, filepath: str, category_pages_chunked: dict) -> int:\n",
    "    \"\"\"\n",
    "    Chunks wiki data for all pages in a category and its subcategories. Downloaded wiki data is available in the\n",
    "    .metadata/download folder. Chunks and hierarchy information are stored in the .metadata/chunk folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_total_pages_chunked = 0\n",
    "    title_pathname = get_title_pathname_map(filepath)\n",
    "    \n",
    "    pages_filename_set = {file.name for file in Path(filepath).glob(\"*.html\")}\n",
    "    categories_dirname_set = {dir.name for dir in Path(filepath).iterdir() if dir.is_dir() and dir.name != \".metadata\"}\n",
    "    \n",
    "    pages = title_pathname[\"pages\"]\n",
    "    for page_title, page_filename in pages.items():\n",
    "        if r.sismember(\"chunked_pages\", page_title):\n",
    "            continue\n",
    "        if page_filename not in pages_filename_set:\n",
    "            continue\n",
    "        chunk_page(filepath, page_title, page_filename)\n",
    "        num_total_pages_chunked += 1\n",
    "        r.sadd(\"chunked_pages\", page_title)\n",
    "    \n",
    "    if num_total_pages_chunked > 0:\n",
    "        category_pages_chunked[category] = num_total_pages_chunked\n",
    "    \n",
    "    subcategories = title_pathname[\"categories\"]\n",
    "    for subcategory_title, subcategory_path in subcategories.items():\n",
    "        if r.sismember(\"chunked_categories\", subcategory_title):\n",
    "            continue\n",
    "        if subcategory_path not in categories_dirname_set:\n",
    "            continue\n",
    "        subcategory_path = os.path.join(filepath, subcategory_path)\n",
    "        subcategory_total_pages_chunked = chunk_wiki_data(subcategory_title, subcategory_path, category_pages_chunked)\n",
    "        num_total_pages_chunked += subcategory_total_pages_chunked\n",
    "        r.sadd(\"chunked_categories\", subcategory_title)\n",
    "    \n",
    "    return num_total_pages_chunked\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_pages_removed = {}\n",
    "num_total_pages_chunked = chunk_wiki_data(\"Dinosaurs\", \"data/v2/Dinosaurs\", category_pages_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chunking is pretty fast!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_pages_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dinosaurs': 11,\n",
       " 'Dinosaur-related lists': 3,\n",
       " 'Dinosaur paleontology': 3,\n",
       " 'Dinosaurs in popular culture': 14,\n",
       " 'Dinosaur taxonomy': 1,\n",
       " 'Ornithischians': 3,\n",
       " 'Saurischians': 11,\n",
       " 'Dinosaur stubs': 63}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_pages_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"chunked_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"chunked_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dinosaurs by geologic time unit\n",
      "Dinosaur-related lists\n",
      "Dinosaurs by location\n",
      "Dinosaur paleontology\n",
      "Dinosaurs in popular culture\n",
      "Dinosaur taxonomy\n",
      "Ornithischians\n",
      "Saurischians\n",
      "Dinosaur stubs\n"
     ]
    }
   ],
   "source": [
    "chunked_categories = r.smembers(\"chunked_categories\")\n",
    "\n",
    "# Print all members\n",
    "for category in chunked_categories:\n",
    "    print(category.decode('utf-8'))  # Decode bytes to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "def remove_metadata(dirnames: List[str], filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Remove metadata from .metadata/ directory. Currently, dirnames list can have one or more of 'download', 'chunk', 'index/embeddings'.\n",
    "    \"\"\"\n",
    "    for dirname in dirnames:\n",
    "        metadata_dir_path = os.path.join(filepath, f\".metadata/{dirname}\")\n",
    "        if os.path.exists(metadata_dir_path):\n",
    "            shutil.rmtree(metadata_dir_path)\n",
    "\n",
    "    categories_dirname_list = [dir.name for dir in Path(filepath).iterdir() if dir.is_dir() and dir.name != \".metadata\"]\n",
    "\n",
    "    for category_dir in categories_dirname_list:\n",
    "        category_dir_path = os.path.join(filepath, category_dir)\n",
    "        remove_metadata(dirnames, category_dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_metadata_and_redis_sets(dirnames: List[str], rsets: List[str], filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Remove metadata from .metadata/ directory and remove Redis sets for chunked pages and categories.\n",
    "    \"\"\"\n",
    "    remove_metadata(dirnames, filepath)\n",
    "    \n",
    "    for rset in rsets:\n",
    "        r.delete(rset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirnames = [\"chunk\", \"index/embeddings\"]\n",
    "rsets = [\"chunked_pages\", \"chunked_categories\", \"indexed_pages\", \"indexed_categories\"]\n",
    "remove_metadata_and_redis_sets(dirnames, rsets, \"data/v2/Dinosaurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"downloaded_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"chunked_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"indexed_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
